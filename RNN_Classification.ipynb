{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs for Text Classification\n",
        "\n",
        "We will use a RNN based model to perform classification of SMS messages into Spam or not Spam. The notebook has split the entire process into several parts for your convienience. To appreciate preprocessing in music, it is important to understand preprocessing in other domains. You may need to read up upon tokenization, embeddings (Glove) and Dataloaders, as well as how to pipeline an end - to - end AI model\n",
        "\n",
        "**Resources:** \\\n",
        "https://www.geeksforgeeks.org/pre-trained-word-embedding-using-glove-in-nlp-models/\n",
        "\n",
        "We do not expect you to finish the code entirely, take help whenever required, but understand the code you have written, do not blindly copy code. In case of help required at any time, feel free to contact the project leads.\n",
        "\n"
      ],
      "metadata": {
        "id": "42xt85VTJ_ev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwYp_PIQjQxv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Spam SMS Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "\n",
        "!unzip /content/smsspamcollection.zip\n",
        "!rm /content/readme\n",
        "!rm !rm /content/smsspamcollection.zip\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "FBvZraf2jYIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the GloVe embeddings database\n",
        "\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "!unzip /content/glove.6B.zip\n",
        "\n",
        "!rm -rf /content/glove.6B.zip\n",
        "!rm /content/glove.6B.100d.txt\n",
        "!rm /content/glove.6B.200d.txt\n",
        "!rm /content/glove.6B.300d.txt\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "enwQxTBKf4S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "label = []\n",
        "\n",
        "\"\"\" read each line of the text file and create a Pandas Data Frame\n",
        "        label spam messages as 1 and legit messages as 0\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/SMSSpamCollection\") as f:\n",
        "  for line in f:\n",
        "    parts = line.split('\\t')\n",
        "    text.append(parts[1].strip())\n",
        "\n",
        "    if parts[0]=='spam':\n",
        "      label.append(1)\n",
        "    else:\n",
        "      label.append(0)\n"
      ],
      "metadata": {
        "id": "JAdzZHGZl3C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Pandas Dataframe\n",
        "sms = pd.DataFrame(zip(text, label), columns = [\"Text\", \"Label\"])\n",
        "\n",
        "sms['Text_Length'] = sms['Text'].apply(len)\n",
        "print(sms.head())"
      ],
      "metadata": {
        "id": "SRuld-70mCdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14e0153-427c-4cdc-a077-dd2938a2fd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text  Label  Text_Length\n",
            "0  Go until jurong point, crazy.. Available only ...      0          111\n",
            "1                      Ok lar... Joking wif u oni...      0           29\n",
            "2  Free entry in 2 a wkly comp to win FA Cup fina...      1          155\n",
            "3  U dun say so early hor... U c already then say...      0           49\n",
            "4  Nah I don't think he goes to usf, he lives aro...      0           61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_tokenizer = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize (text):\n",
        "\n",
        "    \"\"\"remove any non-ascii characters\n",
        "       remove punctuations\n",
        "       tokenize the text\n",
        "       return the tokenized text\n",
        "    \"\"\"\n",
        "\n",
        "    #remove any non-ascii characters and tokenize\n",
        "    text = ''.join([char for char in text if ord(char) < 128])\n",
        "    doc = spacy_tokenizer(text)\n",
        "    #remove punctuations after iterating over the tokens\n",
        "    tokens = [token.text for token in doc if not token.is_punct]\n",
        "    return tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "KzOJgFYCmTPx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text sms in the Pandas Dataframe\n",
        "sms[\"Tokenized_Text\"] = sms['Text'].apply(tokenize)"
      ],
      "metadata": {
        "id": "i5mH9RpmmsEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_GloVe_embeddings(glove_file):\n",
        "\n",
        "    \"\"\"\n",
        "        load the GloVe embeddings from the files downloaded\n",
        "        create a dictionary of the form {word : word embedding}\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings_dict = {}\n",
        "\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        parts = line.split()\n",
        "        word = parts[0]\n",
        "        vector = np.array(parts[1:], dtype= np.float32)\n",
        "        embeddings_dict[word] = vector\n",
        "\n",
        "    return embeddings_dict\n",
        "\n",
        "glove_file = '/content/glove.6B.50d.txt'  # Adjust the path as needed\n",
        "\n",
        "glove_embeddings = load_GloVe_embeddings(glove_file)\n"
      ],
      "metadata": {
        "id": "Vw0txfuIm29P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(tokenized_text, word_embeddings, max_text_length=20, embedding_size = 50):\n",
        "    \"\"\"\n",
        "        given a sequence of tokens convert them to their word embeddings\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((max_text_length, embedding_size), dtype = np.float32)\n",
        "\n",
        "    for i, token in enumerate(tokenized_text[:max_text_length]):\n",
        "      if token in word_embeddings:\n",
        "        embedding_matrix[i] = word_embeddings[token]\n",
        "      else:\n",
        "        embedding_matrix[i] = np.zeros(embedding_size, dtype = np.float32)\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "o-O4DeKRnjL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms[\"Embedded_Text\"] = sms[\"Tokenized_Text\"].apply(lambda tokens: embed_text(tokens, glove_embeddings, max_text_length=20, embedding_size=50))\n"
      ],
      "metadata": {
        "id": "MmOFLtqbn-2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Complete the below code for the Dataloader class\"\"\"\n",
        "class load_dataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        \"\"\"\n",
        "            X: the embeddings of the sentence\n",
        "            Y: ground truth of the sentence (0- positive, 1- negative)\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "WtOWOOv_jYPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, num_layers, num_classes):\n",
        "        \"\"\"Define your layers, activation functions here\"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size = embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform a forward pass\"\"\"\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:,-1,:])\n",
        "        return out"
      ],
      "metadata": {
        "id": "q6fuH7svjYRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(num_epochs, train_loader, model, criterion, optimizer, device):\n",
        "\n",
        "    \"\"\"\n",
        "    Write a trainer loop for the model. It must follow the below pattern\n",
        "    1. Pass the input to the model and perform forward propagation\n",
        "    2. Obtain losses\n",
        "    3. Backpropagate to find the gradients\n",
        "\n",
        "    Make sure to check the accuracy of the model at regular intervals\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "id": "rRlz7uqbjYUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Write code to split your available data into training and testing splits\n",
        "2. Define the model\n",
        "3. Set up hyper-parameters such as learning rate, number of epochs, batch size\n",
        "4. Train the model by using the function you defined above\n",
        "5. Check the model accuracy by running the model on the testing split\n",
        "6. Save the model as a .pth file\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    np.array(sms[\"Embedded_Text\"].tolist()),\n",
        "    sms[\"Label\"].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = load_dataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = load_dataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "embedding_size = 50\n",
        "hidden_size = 100\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "\n",
        "model = RNN(embedding_size, hidden_size, num_layers, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "train_model(num_epochs, train_loader, model, criterion, optimizer,device)\n",
        "\n",
        "torch.save(model.state_dict(), 'rnn_model.pth')\n"
      ],
      "metadata": {
        "id": "Rap_LDx1jYW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48c0854-e7a3-4ae6-e06d-1244025fac1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2806, Accuracy: 89.03%\n",
            "Epoch [2/10], Loss: 0.1814, Accuracy: 93.07%\n",
            "Epoch [3/10], Loss: 0.1691, Accuracy: 93.94%\n",
            "Epoch [4/10], Loss: 0.1559, Accuracy: 94.39%\n",
            "Epoch [5/10], Loss: 0.1600, Accuracy: 94.28%\n",
            "Epoch [6/10], Loss: 0.1557, Accuracy: 94.37%\n",
            "Epoch [7/10], Loss: 0.1468, Accuracy: 95.07%\n",
            "Epoch [8/10], Loss: 0.1292, Accuracy: 95.34%\n",
            "Epoch [9/10], Loss: 0.1234, Accuracy: 95.58%\n",
            "Epoch [10/10], Loss: 0.1056, Accuracy: 96.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence, spacy_tokenizer, word_embeddings, max_text_length=20):\n",
        "    # Tokenize the text\n",
        "    tokens = tokenize(sentence)\n",
        "\n",
        "    # Embed the tokens\n",
        "    embedded_text = embed_text(tokens, word_embeddings, max_text_length=max_text_length)\n",
        "\n",
        "    return torch.tensor(embedded_text, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def predict(sentence, model, spacy_tokenizer, word_embeddings, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Preprocess and embed the input sentence\n",
        "        inputs = preprocess_sentence(sentence, spacy_tokenizer, word_embeddings)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Perform the prediction\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        return predicted.item()\n",
        "\n",
        "# Example usage\n",
        "random_sentence = \"Congratulations! Your credit score entitles you to a no-interest Visa credit card. Click here to claim\"\n",
        "prediction = predict(random_sentence, model, spacy_tokenizer, glove_embeddings, device)\n",
        "\n",
        "print(f\"Prediction: {'Spam' if prediction == 1 else 'Ham'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnXXCe0mTAzC",
        "outputId": "dcd6a4be-bff0-4089-ba16-2950ea9beab2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Spam\n"
          ]
        }
      ]
    }
  ]
}